{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requirements and presentation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# figure plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"figures\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title, normalize=False, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label') \n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fpb = pd.read_csv(\"./data/financial-phrase-bank-v1.0/Sentences_66Agree.txt\", sep='@',encoding='latin-1', names=['Text','Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    Rating\n",
       "0  According to Gran , the company has no plans t...   neutral\n",
       "1  Technopolis plans to develop in stages an area...   neutral\n",
       "2  With the new production plant the company woul...  positive\n",
       "3  According to the company 's updated strategy f...  positive\n",
       "4  For the last quarter of 2010 , Componenta 's n...  positive"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fpb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4217"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_fpb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Changed the getlabel function in binaryprocessor class to have 3 labels, negative, neutral, positive'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Changed the getlabel function in binaryprocessor class to have 3 labels, negative, neutral, positive\"\"\"\n",
    "#df_fpb['Rating'] = df_fpb['Rating'].replace('negative',0)\n",
    "#df_fpb['Rating'] = df_fpb['Rating'].replace('neutral',1)\n",
    "#df_fpb['Rating'] = df_fpb['Rating'].replace('positive',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fpb = sklearn.utils.shuffle(df_fpb, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Tielinja generated net sales of 7.5 mln euro $...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>Cohen &amp; Steers , Inc. : 5 534 626 shares repre...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>SAN FRANCISCO ( MarketWatch ) -- Nokia Corp , ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>Raute said it has won an order worth around 15...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>The power supplies , DC power systems and inve...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>To see a slide show of all the newest product ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Under the rental agreement , Stockmann was com...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>Eero Katajavuori , currently Group Vice Presid...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>The floor area of the Yliopistonrinne project ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>The company , which makes garden tools , sciss...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4217 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text    Rating\n",
       "463   Tielinja generated net sales of 7.5 mln euro $...   neutral\n",
       "2426  Cohen & Steers , Inc. : 5 534 626 shares repre...   neutral\n",
       "2661  SAN FRANCISCO ( MarketWatch ) -- Nokia Corp , ...   neutral\n",
       "1483  Raute said it has won an order worth around 15...  positive\n",
       "2860  The power supplies , DC power systems and inve...   neutral\n",
       "...                                                 ...       ...\n",
       "3444  To see a slide show of all the newest product ...   neutral\n",
       "466   Under the rental agreement , Stockmann was com...   neutral\n",
       "3092  Eero Katajavuori , currently Group Vice Presid...   neutral\n",
       "3772  The floor area of the Yliopistonrinne project ...   neutral\n",
       "860   The company , which makes garden tools , sciss...  positive\n",
       "\n",
       "[4217 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_unprocessed, df_test_unprocessed = train_test_split(df_fpb, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_bert = pd.DataFrame({\n",
    "    'id':range(len(df_train_unprocessed)),\n",
    "    'label':df_train_unprocessed['Rating'],\n",
    "    'alpha':['a']*df_train_unprocessed.shape[0],\n",
    "    'text': df_train_unprocessed['Text']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>Chief Financial Officer Jim Heindlmeyer said B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>a</td>\n",
       "      <td>Danish company FLSmidth has acknowledged that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>a</td>\n",
       "      <td>In a separate announcement to the Helsinki sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>Also Chile is an important market area for for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3538</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>a</td>\n",
       "      <td>In October , UPM reported a third-quarter net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>3368</td>\n",
       "      <td>positive</td>\n",
       "      <td>a</td>\n",
       "      <td>Finnish Aktia Group 's operating profit rose t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>3369</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>Jon Risfelt has previously held operational ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>3370</td>\n",
       "      <td>positive</td>\n",
       "      <td>a</td>\n",
       "      <td>Nokia controls more than 50 percent of phone s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4156</th>\n",
       "      <td>3371</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>`` Low energy consumption and flexible loading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>3372</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>The acquisition of AVC Systemhaus and the majo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3373 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     label alpha                                               text\n",
       "1029     0   neutral     a  Chief Financial Officer Jim Heindlmeyer said B...\n",
       "1651     1  negative     a  Danish company FLSmidth has acknowledged that ...\n",
       "4062     2  negative     a  In a separate announcement to the Helsinki sto...\n",
       "1011     3   neutral     a  Also Chile is an important market area for for...\n",
       "3538     4  negative     a  In October , UPM reported a third-quarter net ...\n",
       "...    ...       ...   ...                                                ...\n",
       "81    3368  positive     a  Finnish Aktia Group 's operating profit rose t...\n",
       "2552  3369   neutral     a  Jon Risfelt has previously held operational ex...\n",
       "763   3370  positive     a  Nokia controls more than 50 percent of phone s...\n",
       "4156  3371   neutral     a  `` Low energy consumption and flexible loading...\n",
       "2700  3372   neutral     a  The acquisition of AVC Systemhaus and the majo...\n",
       "\n",
       "[3373 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_bert = pd.DataFrame({\n",
    "    'id':range(len(df_test_unprocessed)),\n",
    "    'label':df_test_unprocessed['Rating'],\n",
    "    'alpha':['a']*df_test_unprocessed.shape[0],\n",
    "    'text': df_test_unprocessed['Text']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>a</td>\n",
       "      <td>The group 's 12-month operating profit grew 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>s business sectors are building construction ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>Okmetic has used the furnaces for the contract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>JVC will stop producing DVD players in Brazil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>All are welcome .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>839</td>\n",
       "      <td>positive</td>\n",
       "      <td>a</td>\n",
       "      <td>As a result , the distribution companies will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>840</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>The agreement , which will cover monitoring , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>841</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>The unit 's clients are mainly in the field of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>842</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a</td>\n",
       "      <td>Cramo , headquartered in Vantaa , Finland , re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>843</td>\n",
       "      <td>positive</td>\n",
       "      <td>a</td>\n",
       "      <td>Profit after taxes was EUR 0.1 mn , compared t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id     label alpha                                               text\n",
       "357     0  positive     a  The group 's 12-month operating profit grew 31...\n",
       "2657    1   neutral     a  s business sectors are building construction ,...\n",
       "2614    2   neutral     a  Okmetic has used the furnaces for the contract...\n",
       "2303    3   neutral     a  JVC will stop producing DVD players in Brazil ...\n",
       "895     4   neutral     a                                  All are welcome .\n",
       "...   ...       ...   ...                                                ...\n",
       "827   839  positive     a  As a result , the distribution companies will ...\n",
       "1839  840   neutral     a  The agreement , which will cover monitoring , ...\n",
       "3420  841   neutral     a  The unit 's clients are mainly in the field of...\n",
       "2036  842   neutral     a  Cramo , headquartered in Vantaa , Finland , re...\n",
       "1915  843  positive     a  Profit after taxes was EUR 0.1 mn , compared t...\n",
       "\n",
       "[844 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n",
      "844\n",
      "3373\n"
     ]
    }
   ],
   "source": [
    "print(test_df_bert['text'].str.len().max())\n",
    "print(len(test_df_bert))\n",
    "print(len(train_df_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_bert.to_csv('data/train.tsv', sep='\\t', index=False, header=False)\n",
    "test_df_bert.to_csv('data/dev.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "csv.field_size_limit(2147483647) # Increase CSV reader's field limit incase we have long text.\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "\n",
    "class BinaryClassificationProcessor(DataProcessor):\n",
    "    \"\"\"Processor for binary classification dataset.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return ['negative', 'neutral', 'positive']\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            label = line[1]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def convert_example_to_feature(example_row):\n",
    "    # return example_row\n",
    "    example, label_map, max_seq_length, tokenizer, output_mode = example_row\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "    segment_ids = [0] * len(tokens)\n",
    "\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [\"[SEP]\"]\n",
    "        segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    if output_mode == \"classification\":\n",
    "        label_id = label_map[example.label]\n",
    "    elif output_mode == \"regression\":\n",
    "        label_id = float(example.label)\n",
    "    else:\n",
    "        raise KeyError(output_mode)\n",
    "\n",
    "    return InputFeatures(input_ids=input_ids,\n",
    "                         input_mask=input_mask,\n",
    "                         segment_ids=segment_ids,\n",
    "                         label_id=label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tools import *\n",
    "import convert_examples_to_features\n",
    "\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
    "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
    "# bert-base-multilingual-cased, bert-base-chinese.\n",
    "BERT_MODEL = 'bert-base-cased'\n",
    "\n",
    "# The name of the task to train.I'm going to name this 'yelp'.\n",
    "TASK_NAME = 'bert-base-cased-fpb-66'\n",
    "\n",
    "# The output directory where the fine-tuned model and checkpoints will be written.\n",
    "OUTPUT_DIR = f'outputs/{TASK_NAME}/'\n",
    "\n",
    "# The directory where the evaluation reports will be written to.\n",
    "REPORTS_DIR = f'reports/{TASK_NAME}_evaluation_report/'\n",
    "\n",
    "# This is where BERT will look for pre-trained models to load parameters from.\n",
    "CACHE_DIR = 'cache/'\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "EVAL_BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "RANDOM_SEED = 42\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WARMUP_PROPORTION = 0.1\n",
    "OUTPUT_MODE = 'classification'\n",
    "\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = OUTPUT_MODE\n",
    "\n",
    "cache_dir = CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n",
    "        REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
    "        os.makedirs(REPORTS_DIR)\n",
    "if not os.path.exists(REPORTS_DIR):\n",
    "    os.makedirs(REPORTS_DIR)\n",
    "    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
    "    os.makedirs(REPORTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(OUTPUT_DIR))\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3373\n"
     ]
    }
   ],
   "source": [
    "processor = BinaryClassificationProcessor()\n",
    "train_examples = processor.get_train_examples(DATA_DIR)\n",
    "train_examples_len = len(train_examples)\n",
    "print(train_examples_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_optimization_steps = int(\n",
    "    train_examples_len / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/Roshan/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "train_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in train_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to convert 3373 examples..\n",
      "Spawning 15 processes..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-2dd9a213f4ee>:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  train_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, train_examples_for_processing), total=train_examples_len))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f8399f8d8845ebaa58a3bd2131520c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3373.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'neutral'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/Roshan/Library/Mobile Documents/com~apple~CloudDocs/Documents/Year 3 Uni notes/Individual project/individual-project/convert_examples_to_features.py\", line 70, in convert_example_to_feature\n    label_id = label_map[example.label]\nKeyError: 'neutral'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2dd9a213f4ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Spawning {process_count} processes..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_example_to_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_examples_for_processing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_examples_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'neutral'"
     ]
    }
   ],
   "source": [
    "process_count = cpu_count() - 1\n",
    "if __name__ ==  '__main__':\n",
    "    print(f'Preparing to convert {train_examples_len} examples..')\n",
    "    print(f'Spawning {process_count} processes..')\n",
    "    with Pool(process_count) as p:\n",
    "        train_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, train_examples_for_processing), total=train_examples_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + \"train_features_noprocessing.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file cache/bertmultilingual-mlt--link-removed-humor-dropped.tar.gz\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file cache/bertmultilingual-mlt--link-removed-humor-dropped.tar.gz to temp dir /var/folders/_x/_yxng70563d8kg_c7r1h9l680000gn/T/tmpaqepnhk4\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForSequenceClassification.from_pretrained(CACHE_DIR + BERT_MODEL, cache_dir=CACHE_DIR, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=LEARNING_RATE,\n",
    "                     warmup=WARMUP_PROPORTION,\n",
    "                     t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:***** Running training *****\n",
      "INFO:root:  Num examples = 10133\n",
      "INFO:root:  Batch size = 24\n",
      "INFO:root:  Num steps = 422\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", train_examples_len)\n",
    "logger.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
    "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-137-ea91c5942ab2>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ee9e1b2278433ea82eb4fc53fdef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=423.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.379284"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019436"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [2:38:35<00:00, 9515.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "        if OUTPUT_MODE == \"classification\":\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "        elif OUTPUT_MODE == \"regression\":\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        loss.backward()\n",
    "        print(\"\\r%f\" % loss, end='')\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/mlt multilingual bert links removed humour dropped epoch 1.25/vocab.txt'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "# Model and config file saved\n",
    "output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(OUTPUT_DIR, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from tools import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import convert_examples_to_features\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "\n",
    "#     For model evaluation \n",
    "\n",
    "###\n",
    "\n",
    "# The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
    "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
    "# bert-base-multilingual-cased, bert-base-chinese.\n",
    "# IN CACHE FOLDER\n",
    "BERT_MODEL = 'mlt-multilingual-bert-links-removed-humour-dropped-epoch-1.25.tar.gz'\n",
    "\n",
    "# The name of the task to train.\n",
    "# IN OUTPUT FOLDER\n",
    "TASK_NAME = 'mlt multilingual bert links removed humour dropped epoch 1.25'\n",
    "\n",
    "# The output directory where the fine-tuned model and checkpoints will be written.\n",
    "OUTPUT_DIR = f'outputs/{TASK_NAME}/'\n",
    "\n",
    "# The directory where the evaluation reports will be written to.\n",
    "REPORTS_DIR = f'reports/{TASK_NAME}_evaluation_reports/'\n",
    "\n",
    "# This is where BERT will look for pre-trained models to load parameters from.\n",
    "CACHE_DIR = 'cache/'\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "RANDOM_SEED = 42\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WARMUP_PROPORTION = 0.1\n",
    "OUTPUT_MODE = 'classification'\n",
    "\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n",
    "        REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
    "        os.makedirs(REPORTS_DIR)\n",
    "if not os.path.exists(REPORTS_DIR):\n",
    "    os.makedirs(REPORTS_DIR)\n",
    "    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
    "    os.makedirs(REPORTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_report(task_name, labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    bert_classification_report = classification_report(labels, preds)\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"classification report\" : bert_classification_report\n",
    "    }\n",
    "\n",
    "def compute_metrics(task_name, labels, preds):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(task_name, labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file outputs/mlt multilingual bert links removed humour dropped epoch 1.25/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(OUTPUT_DIR + 'vocab.txt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BinaryClassificationProcessor()\n",
    "eval_examples = processor.get_dev_examples(DATA_DIR)\n",
    "label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "num_labels = len(label_list)\n",
    "eval_examples_len = len(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "eval_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to convert 3781 examples..\n",
      "Spawning 15 processes..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-151-e5327a65c549>:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  eval_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, eval_examples_for_processing), total=eval_examples_len))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b71f62c51d47afbb754f21da297a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_count = cpu_count() - 1\n",
    "if __name__ ==  '__main__':\n",
    "    print(f'Preparing to convert {eval_examples_len} examples..')\n",
    "    print(f'Spawning {process_count} processes..')\n",
    "    with Pool(process_count) as p:\n",
    "        eval_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, eval_examples_for_processing), total=eval_examples_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_MODE == \"classification\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file cache/mlt-multilingual-bert-links-removed-humour-dropped-epoch-1.25.tar.gz\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file cache/mlt-multilingual-bert-links-removed-humour-dropped-epoch-1.25.tar.gz to temp dir /var/folders/_x/_yxng70563d8kg_c7r1h9l680000gn/T/tmp3c304k1m\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertForSequenceClassification.from_pretrained(CACHE_DIR + BERT_MODEL, cache_dir=CACHE_DIR, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-156-3f80049d0857>:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedd9ece33cb4caa8398b4031f7bc064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=473.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:***** Eval results *****\n",
      "INFO:root:  task = mlt multilingual bert links removed humour dropped epoch 1.25\n",
      "INFO:root:  mcc = 0.7807881724572309\n",
      "INFO:root:  tp = 2415\n",
      "INFO:root:  tn = 1008\n",
      "INFO:root:  fp = 209\n",
      "INFO:root:  fn = 149\n",
      "INFO:root:  classification report =               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      1217\n",
      "           1       0.92      0.94      0.93      2564\n",
      "\n",
      "    accuracy                           0.91      3781\n",
      "   macro avg       0.90      0.89      0.89      3781\n",
      "weighted avg       0.90      0.91      0.90      3781\n",
      "\n",
      "INFO:root:  eval_loss = 0.29714586671508736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    # create eval loss and other metric required by the task\n",
    "    if OUTPUT_MODE == \"classification\":\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "    elif OUTPUT_MODE == \"regression\":\n",
    "        loss_fct = MSELoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    nb_eval_steps += 1\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "preds = preds[0]\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    preds = np.squeeze(preds)\n",
    "result = compute_metrics(TASK_NAME, all_label_ids.numpy(), preds)\n",
    "\n",
    "result['eval_loss'] = eval_loss\n",
    "\n",
    "output_eval_file = os.path.join(REPORTS_DIR, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in (result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cm_BERT = metrics.confusion_matrix(all_label_ids, preds, labels = [1,0])\n",
    "plot_confusion_matrix(cm_BERT, classes=['fake', 'real'], title = 'BERT No Pre-process Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      1217\n",
      "           1       0.92      0.94      0.93      2564\n",
      "\n",
      "    accuracy                           0.91      3781\n",
      "   macro avg       0.90      0.89      0.89      3781\n",
      "weighted avg       0.90      0.91      0.90      3781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_label_ids ,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8778824941000927"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(all_label_ids, preds, average='micro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
